{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzTQZ_itJ8hK"
      },
      "source": [
        "# Homework and bakeoff: Multi-domain sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hx5sGkq8N9Kc"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Christopher Potts\"\n",
        "__version__ = \"CS224u, Stanford, Spring 2023\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbBTOn0QN9Kd"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cgpotts/cs224u/blob/main/hw_sentiment.ipynb)\n",
        "\n",
        "If Colab is opened with this badge, please **save a copy to drive** (from the File menu) before running the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smL2tgkuN9Kd"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aumeo2huN9Kd"
      },
      "source": [
        "This homework and associated bakeoff are devoted to supervised sentiment analysis in a ternary label setting (positive, negative, neutral). Your ultimate goal is to develop systems that can make accurate predictions in multiple domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4V5aTtIN9Kd"
      },
      "source": [
        "The homework questions ask you to implement some baseline systems using DynaSent Round 1, DynaSent Round 2, and the Stanford Sentiment Treebank. The bakeoff challenge is to define a system that does well on the DynaSent test sets, the SST-3 test set, and a set of mystery examples that don't correspond to the DynaSent or SST-3 domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Vahy_wN9Ke"
      },
      "source": [
        "__Important methodological note:__ The DynaSent and SST-3 test sets are already publicly distributed, so we are counting on people not to cheat by developing their models on these test sets. You must do all your development without using these test sets at all, and then evaluate exactly once on the test set and turn in the results, with no further system tuning or additional runs. _Much of the scientific integrity of our field depends on people adhering to this honor code._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVqY09RDN9Ke"
      },
      "source": [
        "This notebook briefly introduces our three development datasets, states the homework questions, and then provides guidance on the original system and associated bakeoff entry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsLcqWtBJ8hM"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2nFUcNIhJ8hM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Sort of randomly chosen import to see whether the requirements\n",
        "    # are met:\n",
        "    import datasets\n",
        "except ModuleNotFoundError:\n",
        "    !git clone https://github.com/cgpotts/cs224u/\n",
        "    !pip install -r cs224u/requirements.txt\n",
        "    import sys\n",
        "    sys.path.append(\"cs224u\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pyAzJmyYSNMP"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdJba1bGJ8hN"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGAwU8KmJ8hN"
      },
      "source": [
        "### DynaSent round 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8JiUxMGN9Kf"
      },
      "source": [
        "The DynaSent dataset of [Potts, Wu, et al. 2021](https://aclanthology.org/2021.acl-long.186/) is a ternary sentiment benchmark consisting of two rounds (so far). The dataset is available on [Hugging Face](https://huggingface.co/datasets/dynabench/dynasent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUXkNm5uN9Kf"
      },
      "source": [
        "For Round 1, the authors collected sentences from the [Yelp Academic Dataset](https://www.yelp.com/dataset) that fooled a top-performing sentiment model but were intuitive for humans. The model was used only to heuristically find the examples. Crowdworkers multiply-labeled all of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RosWMQNKN9Kf"
      },
      "source": [
        "The round contains a lot of metadata that could be useful for developing sentiment models. We will focus on just the sentences and labels, but you are free to make use of this additional metadata in developing uour system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9UcNgx_SZDG",
        "outputId": "c67d77e7-2dbf-461d-ccc3-9fe79e97b3ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "dynasent_r1 = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r1.all', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7iN05xzSkKo",
        "outputId": "a42ad2e2-2d19-43aa-d9a3-1a706fff0c0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
              "        num_rows: 80488\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
              "        num_rows: 3600\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
              "        num_rows: 3600\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dynasent_r1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_iWixX6J8hO"
      },
      "source": [
        "Splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RVjcdb_4SrS2"
      },
      "outputs": [],
      "source": [
        "def print_label_dist(dataset, labelname='gold_label', splitnames=('train', 'validation')):\n",
        "    for splitname in splitnames:\n",
        "        print(splitname)\n",
        "        dist = sorted(Counter(dataset[splitname][labelname]).items())\n",
        "        for k, v in dist:\n",
        "            print(f\"\\t{k:>14s}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3RQdIBRJ8hP",
        "outputId": "6960b88c-791d-47df-e31a-4761d574356f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "\t      negative: 14021\n",
            "\t       neutral: 45076\n",
            "\t      positive: 21391\n",
            "validation\n",
            "\t      negative: 1200\n",
            "\t       neutral: 1200\n",
            "\t      positive: 1200\n"
          ]
        }
      ],
      "source": [
        "print_label_dist(dynasent_r1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4WFt0C6J8hP"
      },
      "source": [
        "### DynaSent round 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7ypTyq7N9Kg"
      },
      "source": [
        "DynaSent Round 2 was created using different methods than Round 1. For Round 2, crowdworkers edited sentences from the Yelp Academic Dataset seeking to achieve a particular sentiment goal (e.g., expressing a positive sentiment) while fooling a top-performing model. This work was done on the [Dynabench](https://dynabench.org) platform. The hope is that this directly adversarial goal will lead to examples that are very hard for present-day models but intuitive for humans. All the examples were multiply-labeled by separate annotators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz3F_lviJ8hP",
        "outputId": "bc0356af-0858-4813-9696-161f4a37a9a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "dynasent_r2 = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r2.all', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up68q68dJ8hP",
        "outputId": "3a932c52-bd3d-43dc-a5be-bbe84938883e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "\t      negative: 4579\n",
            "\t       neutral: 2448\n",
            "\t      positive: 6038\n",
            "validation\n",
            "\t      negative: 240\n",
            "\t       neutral: 240\n",
            "\t      positive: 240\n"
          ]
        }
      ],
      "source": [
        "print_label_dist(dynasent_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeONNIJQJ8hP"
      },
      "source": [
        "### Stanford Sentiment Treebank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5bpofB_N9Kg"
      },
      "source": [
        "The [Stanford Sentiment Treebank (SST)](http://nlp.stanford.edu/sentiment/) of [Socher et al. 2013](https://aclanthology.org/D13-1170/) is a widely-used resource for evaluating supervised models. It consists of sentences from Rotten Tomatoes Movie Reviews (see [Pang and Lee's project page](https://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.home.html)). We will use the ternary version of the task (SST-3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o-CZ3rvN9Kg"
      },
      "source": [
        "SST examples are special in that they are labeled at the phrase-level as well as the sentence level, which provides very extensive and detailed supervision for sentiment. We will use only the sentence-level labels for the homework, but you are free to use the phrase-level labels as well in designing your original system. (To do this, you will need to get the dataset from the above project page, since the Hugging Face SST-3 we are using does not include these labels.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJROF2MaJ8hP",
        "outputId": "44645668-711c-4854-d26e-ee7a90995499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        }
      ],
      "source": [
        "sst = load_dataset(\"SetFit/sst5\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw0I60nOJ8hQ",
        "outputId": "06c49b62-c19f-463f-a5be-1830f717ebe3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 8544\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 1101\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 2210\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "sst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPGjvCjkN9Kh"
      },
      "source": [
        "Out of the box, this is a five-way task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxmbH6qkJ8hQ",
        "outputId": "95991520-18ee-40b4-92c7-f352a268707b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "\t      negative: 2218\n",
            "\t       neutral: 1624\n",
            "\t      positive: 2322\n",
            "\t very negative: 1092\n",
            "\t very positive: 1288\n",
            "validation\n",
            "\t      negative: 289\n",
            "\t       neutral: 229\n",
            "\t      positive: 279\n",
            "\t very negative: 139\n",
            "\t very positive: 165\n"
          ]
        }
      ],
      "source": [
        "print_label_dist(sst, labelname='label_text')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oFrTjoRN9Ki"
      },
      "source": [
        "The above labels are not aligned with our ternary task, and the dataset distribution uses slightly different keys from those of DynaSent. The following code converts the dataset to SST-3 and also aligns the dataset keys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WDgWI8IzJ8hQ"
      },
      "outputs": [],
      "source": [
        "def convert_sst_label(s):\n",
        "    return s.split(\" \")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UDT2FS0RJ8hQ"
      },
      "outputs": [],
      "source": [
        "for splitname in ('train', 'validation', 'test'):\n",
        "    dist = [convert_sst_label(s) for s in sst[splitname]['label_text']]\n",
        "    sst[splitname] = sst[splitname].add_column('gold_label', dist)\n",
        "    sst[splitname] = sst[splitname].add_column('sentence', sst[splitname]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-dl9asPJ8hQ",
        "outputId": "e6293583-143f-4bee-a6f9-d921001a663b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "\t      negative: 3310\n",
            "\t       neutral: 1624\n",
            "\t      positive: 3610\n",
            "validation\n",
            "\t      negative: 428\n",
            "\t       neutral: 229\n",
            "\t      positive: 444\n"
          ]
        }
      ],
      "source": [
        "print_label_dist(sst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjHYndSJN9Kj"
      },
      "source": [
        "## Question 1: Linear classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHPgdpfuJ8hQ"
      },
      "source": [
        "Our first set of experiments will use simple linear classifiers with sparse representations derived from counting unigrams. These experiments will introduce some useful techniques and provide a baseline for original systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDVlQaNqN9Kj"
      },
      "source": [
        "### Background: Feature functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKIQD6G2N9Kj"
      },
      "source": [
        "The following is a flexible format for writing feature functions in the context of scikit-learn modeling. The function maps a string to a count dictionary, using the simple procedure of splitting on whitespace and counting the resulting elements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "evJ2VaInN9Kk"
      },
      "outputs": [],
      "source": [
        "def unigrams_phi(s):\n",
        "    \"\"\"The basis for a unigrams feature function.\n",
        "\n",
        "    Downcases all tokens.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    s : str\n",
        "        The example to represent\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Counter\n",
        "        A map from tokens (str) to their counts in `text`\n",
        "\n",
        "    \"\"\"\n",
        "    return Counter(s.lower().split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-RGqOiWN9Kk"
      },
      "source": [
        "Quick example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtL2CYFtN9Kk",
        "outputId": "fba6bf64-a419-465a-b2c6-80829418317a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({\"here's\": 1,\n",
              "         'an': 2,\n",
              "         'example': 1,\n",
              "         'with': 1,\n",
              "         'emoticon': 1,\n",
              "         ':)!': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "unigrams_phi(\"Here's an example with an emoticon :)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeLG-xM5N9Ko"
      },
      "source": [
        "### Background: Feature space vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGqL6fIAN9Kp"
      },
      "source": [
        "Functions like `unigrams_phi`  are just the __basis__ for feature representations. In truth, our models typically don't represent examples as dictionaries, but rather as vectors embedded in a matrix. In general, to manage the translation from dictionaries to vectors, we use [sklearn.feature_extraction.DictVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) instances. Here's a brief overview of how these work:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKtEKH27N9Kp"
      },
      "source": [
        "To start, suppose that we had just two examples to represent, and our feature function mapped them to the following list of dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fxQBfC4zN9Kp"
      },
      "outputs": [],
      "source": [
        "train_feats = [\n",
        "    {'a': 1, 'b': 1},\n",
        "    {'b': 1, 'c': 2}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4S4N8pbN9Kp"
      },
      "source": [
        "Now we create a `DictVectorizer`. So that we can more easily inspect the resulting matrix, I've set `sparse=False`, so that the return value is a dense matrix. For real problems, you'll probably want to use `sparse=True`, as it will be vastly more efficient for the very sparse feature matrices that you are likely to be creating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Zj-lLARYN9Kp"
      },
      "outputs": [],
      "source": [
        "vec = DictVectorizer(sparse=False)  # Use `sparse=True` for real problems!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTC8pQ9dN9Kp"
      },
      "source": [
        "The `fit_transform` method maps our list of dictionaries to a matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZGmR8CbTN9Kp"
      },
      "outputs": [],
      "source": [
        "X_train = vec.fit_transform(train_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDagc0JCN9Kp"
      },
      "source": [
        "Here I'll create a `pd.Datafame` just to help us inspect `X_train`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "RcgEtg-4N9Kp",
        "outputId": "0a83d2b4-5e0f-407b-9cf9-a43e214a6db5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     a    b    c\n",
              "0  1.0  1.0  0.0\n",
              "1  0.0  1.0  2.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1e6a770e-81ef-4406-a0d5-15a5b509cc45\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e6a770e-81ef-4406-a0d5-15a5b509cc45')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1e6a770e-81ef-4406-a0d5-15a5b509cc45 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1e6a770e-81ef-4406-a0d5-15a5b509cc45');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc3b26fc-bf16-4e83-b291-d63d40844d46\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc3b26fc-bf16-4e83-b291-d63d40844d46')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc3b26fc-bf16-4e83-b291-d63d40844d46 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"a\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7071067811865476,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"b\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"c\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4142135623730951,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "pd.DataFrame(X_train, columns=vec.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqZ-5whaN9Kp"
      },
      "source": [
        "Now we can see that, intuitively, the feature called \"a\" is embedded in the first column, \"b\" in the second column, and \"c\" in the third."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n27D6Q3WN9Kp"
      },
      "source": [
        "Now suppose we have some new test examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8m0pWb5_N9Kq"
      },
      "outputs": [],
      "source": [
        "test_feats = [\n",
        "    {'a': 2, 'c': 1},\n",
        "    {'a': 4, 'b': 2, 'd': 1}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9C2U5D9N9Kq"
      },
      "source": [
        "If we have trained a model on `X_train`, then it will not have any way to deal with this new feature \"d\". This shows that we need to embed `test_feats` in the same space as `X_train`. To do this, one just calls `transform` on the existing vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "arsfECaqN9Kq"
      },
      "outputs": [],
      "source": [
        "X_test = vec.transform(test_feats)  # Not `fit_transform`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "aJhUuOwbN9Kq",
        "outputId": "903c1fa6-4adf-4d6b-8eed-8fc0dedb1d2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     a    b    c\n",
              "0  2.0  0.0  1.0\n",
              "1  4.0  2.0  0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dadfaa12-fefd-4ce8-b7f0-cd73aa07b365\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dadfaa12-fefd-4ce8-b7f0-cd73aa07b365')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dadfaa12-fefd-4ce8-b7f0-cd73aa07b365 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dadfaa12-fefd-4ce8-b7f0-cd73aa07b365');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-aa93e701-6fcb-414b-9313-5edb7be54d0b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa93e701-6fcb-414b-9313-5edb7be54d0b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-aa93e701-6fcb-414b-9313-5edb7be54d0b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"a\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4142135623730951,\n        \"min\": 2.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"b\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4142135623730951,\n        \"min\": 0.0,\n        \"max\": 2.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"c\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7071067811865476,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "pd.DataFrame(X_test, columns=vec.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Eh-ApmYN9Kq"
      },
      "source": [
        "The most common mistake with `DictVectorizer` is calling `fit_transform` on test examples. This will wipe out the existing representation scheme, replacing it with one that matches the test examples. That will happen silently, but then you'll find that the new representations are incompatible with the model you fit. This is likely to manifest itself as a `ValueError` relating to feature counts. Here's an example that might help you spot this if and when it arises in your own work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4rwcMiXkN9Kq"
      },
      "outputs": [],
      "source": [
        "toy_mod = LogisticRegression()\n",
        "\n",
        "vec = DictVectorizer(sparse=False)\n",
        "\n",
        "X_train = vec.fit_transform(train_feats)\n",
        "\n",
        "toy_mod.fit(X_train, [0, 1])\n",
        "\n",
        "# Here's the error! Don't use `fit_transform` again!\n",
        "# Use `transform`!\n",
        "X_test = vec.transform(test_feats)\n",
        "\n",
        "try:\n",
        "    toy_mod.predict(X_test)\n",
        "except ValueError as err:\n",
        "    print(\"ValueError: {}\".format(err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apAAC6osN9Kq"
      },
      "source": [
        "### Background: scikit-learn models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D47yH6dRN9Kq"
      },
      "source": [
        "scikit-learn is an amazing package with, among many other things, an incredible array of classifier model implementations. We're going to use a simple softmax classifier for this homework question, but you will find that you can swap in essentially any scikit-learn classifier and see how it does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTxkou1rN9Kq"
      },
      "source": [
        "The core rhythm for scikit-learn models:\n",
        "\n",
        "1. Instantiate the model with any hyperparamters.\n",
        "2. `fit`\n",
        "3. `predict`\n",
        "\n",
        "Here's a quick example that also shows off scikit-learn's functionality for creating synthetic datasets and random train/test splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vgQZcXlMN9Kq"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_toy, y_toy = make_classification(\n",
        "    n_samples=200, n_classes=3,\n",
        "    n_informative=15, n_features=20,\n",
        "    weights=[0.2, 0.2, 0.6],\n",
        "    random_state=1)\n",
        "\n",
        "X_toy_train, X_toy_test, y_toy_train, y_toy_test = train_test_split(\n",
        "    X_toy, y_toy, test_size=0.20, stratify=y_toy, random_state=1)\n",
        "\n",
        "toymod = LogisticRegression(penalty='l2', C=1, fit_intercept=True)\n",
        "\n",
        "toymod.fit(X_toy_train, y_toy_train)\n",
        "\n",
        "toypreds = toymod.predict(X_toy_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoI-otaxN9Kq"
      },
      "source": [
        "### Background: Classifier assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUfF6XLQN9Kq"
      },
      "source": [
        "When assessing a classifier, the best first step is usually to get a classification report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXytXPJ0N9Kq",
        "outputId": "bee64e32-0a19-4e59-8e51-bb4f93aa01d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.444     0.500     0.471         8\n",
            "           1      0.444     0.500     0.471         8\n",
            "           2      0.909     0.833     0.870        24\n",
            "\n",
            "    accuracy                          0.700        40\n",
            "   macro avg      0.599     0.611     0.604        40\n",
            "weighted avg      0.723     0.700     0.710        40\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_toy_test, toypreds, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Ou1m0bN9Kq"
      },
      "source": [
        "In this course, we will generally focus in the __macro-average F1 score__ (macro avg above). This is simply the mean of the per-class F1 scores, without any attention paid to the overall size of the class. This is our default because, in NLP, we tend to care about small classes as much as (often more than) large classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqiIZmf4N9Kq"
      },
      "source": [
        "The scikit-learn implementation of `macro_f1` can be finicky, so our course code provides a convenient wrapper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "1lLN1avMN9Kq",
        "outputId": "add82b3c-4473-431f-b901-e087aeafc1f9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-f8e33d37ff47>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_macro_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_toy_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoypreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import utils\n",
        "\n",
        "utils.safe_macro_f1(y_toy_test, toypreds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBN6_6IRN9Kr"
      },
      "source": [
        "Note: scikit-learn models have a `score` method. For classifiers, this is set to use `accuracy` by default:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj5E6hMON9Kr"
      },
      "outputs": [],
      "source": [
        "toymod.score(X_toy_test, y_toy_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Lw4N5NON9Kr"
      },
      "source": [
        "Accuracy generally isn't well-aligned with our goals, so we discourage use of this method (and of accuracy scores in general)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlrSxmv8N9Kr"
      },
      "source": [
        "scikit-learn also makes it very easy to perform automatic hyperparameter tuning. A quick example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-S7T5uQN9Kr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'C': (0.1, 0.2, 0.3), 'fit_intercept': [True, False]}\n",
        "\n",
        "toymod_tuned = LogisticRegression()\n",
        "\n",
        "clf = GridSearchCV(toymod_tuned, params, scoring='f1_macro')\n",
        "\n",
        "_ = clf.fit(X_toy, y_toy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxfqNbu5N9Kr"
      },
      "source": [
        "Here's the best model found by this search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wig-BkcdN9Kr"
      },
      "outputs": [],
      "source": [
        "clf.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTbdjwslN9Kr"
      },
      "source": [
        "Because we set `scoring='f1_macro'`, the above model was selected using our favored classifier scoring metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6lBWgOjN9Kr"
      },
      "outputs": [],
      "source": [
        "clf.best_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0qd9DKvN9Kr"
      },
      "source": [
        "With this best model in hand, we can perform our usual assessment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyjbZ6dDN9Kr"
      },
      "outputs": [],
      "source": [
        "bestpreds = clf.best_estimator_.predict(X_toy_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YqbFj-oN9Kr"
      },
      "outputs": [],
      "source": [
        "print(classification_report(bestpreds, y_toy_test, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXg9prmN9Kr"
      },
      "source": [
        "### Task 1: Feature functions [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS7mvqaRN9Kr"
      },
      "source": [
        "The tokenization scheme used by `unigrams_phi` is very basic and leads to unintuitive tokens with punctuation attached to them. Your task here is to complete `tweetgrams_phi`, which should lead to more intuitive results. The task is really just to use the NLTK [TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.casual.html#nltk.tokenize.casual.TweetTokenizer) in place of the simple whitespace tokenization of `unigrams_phi` above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNX37JLQN9Kr"
      },
      "outputs": [],
      "source": [
        "# Your `tweetgrams_phi` should tokenize data according to this tokenizer from NLTK:\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "def tweetgrams_phi(s, **kwargs):\n",
        "    \"\"\"The basis for a feature function using `TweetTokenizer`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    s : str\n",
        "    kwargs : dict\n",
        "        Passed to `TweetTokenizer`\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Counter\n",
        "        A map from tokens to their counts in `text`\n",
        "\n",
        "    \"\"\"\n",
        "    tokenizer = TweetTokenizer(**kwargs)\n",
        "    tokens = tokenizer.tokenize(s)\n",
        "    return Counter(tokens)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UfUHD2pN9Ks"
      },
      "source": [
        "Here's a test you can use to check that your implementation is correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G05v73BN9Ks"
      },
      "outputs": [],
      "source": [
        "def test_tweetgrams_phi(func):\n",
        "    examples = [\n",
        "        (\n",
        "            \"Here's an example with an emoticon :)\",\n",
        "            Counter({'an': 2, \"Here's\": 1, 'example': 1, 'with': 1, 'emoticon': 1, ':)': 1})\n",
        "        ),\n",
        "        (\n",
        "            \"The URL is https://pytorch.org!\",\n",
        "            Counter({'The': 1, 'URL': 1, 'is': 1, 'https://pytorch.org': 1, '!': 1})\n",
        "        )\n",
        "    ]\n",
        "    errcount = 0\n",
        "    for ex, expected in examples:\n",
        "        result = func(ex, preserve_case=True)\n",
        "        if result != expected:\n",
        "            errcount += 1\n",
        "            print(f\"Error for `{func.__name__}`: For input {ex}, \"\n",
        "                  f\"expected {expected} but got {result}\")\n",
        "    caps_ex = \"CAPS\"\n",
        "    caps_result = func(caps_ex, preserve_case=False)\n",
        "    caps_expected = Counter({\"caps\": 1})\n",
        "    if caps_result != caps_expected:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: For input {caps_ex}, \"\n",
        "              f\"expected {caps_expected} but got {caps_result}\")\n",
        "    if errcount == 0:\n",
        "        print(f\"All tests passed for `{func.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4Pbo-STN9Ks"
      },
      "outputs": [],
      "source": [
        "test_tweetgrams_phi(tweetgrams_phi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uIlGqZGJ8hR"
      },
      "source": [
        "### Task 2: Model training [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mvac10zN9Ks"
      },
      "source": [
        "Your task is to complete `train_linear_model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtEaAkHtStQg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfhADdcyJ8hR"
      },
      "outputs": [],
      "source": [
        "def train_linear_model(model, featfunc, train_dataset):\n",
        "    \"\"\"Train an sklearn classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : sklearn classifier model\n",
        "    featfunc : func\n",
        "        Maps strings to Counter instances\n",
        "    train_dataset: dict\n",
        "        Must have a key \"sentence\" containing strings that `featfunc`\n",
        "        will process, and a key \"gold_label\" giving labels\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        * A trained version of `model`\n",
        "        * A fitted `vectorizer` for the train set\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Featurize all the examples in `train_dataset['sentence']`\n",
        "    feature_dicts = [featfunc(sentence) for sentence in train_dataset['sentence']]\n",
        "\n",
        "\n",
        "\n",
        "    # Step 2: Instantiate and use a `DictVectorizer`:\n",
        "    vectorizer = DictVectorizer()\n",
        "    X_train = vectorizer.fit_transform(feature_dicts)\n",
        "\n",
        "\n",
        "    # Step 3: Train the model on the feature matrix and\n",
        "    # train_dataset['gold_label']:\n",
        "    model.fit(X_train, train_dataset['gold_label'])\n",
        "\n",
        "\n",
        "\n",
        "    # Step 4: Return (model, vectorizer):\n",
        "    return model, vectorizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVZyDoHaN9Ks"
      },
      "source": [
        "You can use the following test to help ensure that your implementation is correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdQn0s25N9Ks"
      },
      "outputs": [],
      "source": [
        "def test_train_linear_model(func):\n",
        "    train_dataset = {\n",
        "        'sentence': ['A A', 'A B', 'B B', 'B A', 'B'],\n",
        "        'gold_label': [0, 1, 0, 1, 1]}\n",
        "    def featfunc(s):\n",
        "        return Counter(s.split())\n",
        "    model = LogisticRegression()\n",
        "    result = func(model, featfunc, train_dataset)\n",
        "    if not isinstance(result, tuple) or len(result) != 2:\n",
        "        print(f\"Error for `{func.__name__}`: Incorrect return type\")\n",
        "        return\n",
        "    model, vectorizer = result\n",
        "    if not hasattr(vectorizer, 'vocabulary_'):\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Second return value is not a trained vectorizer\")\n",
        "        return\n",
        "    if not hasattr(model, 'classes_'):\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"First return value is not a trained classifier\")\n",
        "        return\n",
        "    print(f\"No errors found for `{func.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESZEfL0IN9Ks"
      },
      "outputs": [],
      "source": [
        "_ = test_train_linear_model(train_linear_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUeiAZk7N9Ks"
      },
      "source": [
        "You can now very easily train models on our datasets. Quick example (this shouldn't take more than a couple of minutes to run even on a CPU):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQoCRRPHJ8hR"
      },
      "outputs": [],
      "source": [
        "lr_unigrams, vec_unigrams = train_linear_model(\n",
        "    LogisticRegression(max_iter=1000),\n",
        "    unigrams_phi, dynasent_r1['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DaVkdmMN9Ks"
      },
      "source": [
        "### Task 3: Model assessment [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU2k0Vu6N9Ks"
      },
      "source": [
        "Having now trained a model, we'd like to perform assessments on new data. Your task is to complete the wrapper function `assess_linear_model` to do this. The primary things you need to put into practice are (1) how to use a trained vectorizer on new data and (2) how to make predictions with your trained model. (Both of these steps are reviewed earlier in this notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKa7uOwYJ8hR"
      },
      "outputs": [],
      "source": [
        "def assess_linear_model(model, featfunc, vectorizer, assess_dataset):\n",
        "    \"\"\"Assess a trained sklearn model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: trained sklearn model\n",
        "    featfunc : func\n",
        "        Maps strings to count dicts\n",
        "    vectorizer : fitted DictVectorizer\n",
        "    assess_dataset: dict\n",
        "        Must have a key \"sentence\" containing strings that `featfunc`\n",
        "        will process, and a key \"gold_label\" giving labels\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A classification report (multiline string)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Featurize the assessment data:\n",
        "    feature_dicts = [featfunc(sentence) for sentence in assess_dataset['sentence']]\n",
        "\n",
        "\n",
        "\n",
        "    # Step 2: Vectorize the assessment data features:\n",
        "    X_assess = vectorizer.transform(feature_dicts)\n",
        "\n",
        "\n",
        "\n",
        "    # Step 3: Make predictions:\n",
        "    predictions = model.predict(X_assess)\n",
        "\n",
        "\n",
        "\n",
        "    # Step 4: Return a classification report (str):\n",
        "    return classification_report(assess_dataset['gold_label'], predictions)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbfIwdLxN9Kt"
      },
      "source": [
        "Here's a quick test you can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyeO_aFiN9Kt"
      },
      "outputs": [],
      "source": [
        "def test_assess_linear_model(assessfunc, trainfunc):\n",
        "    train_dataset = {\n",
        "        'sentence': ['A A', 'A B', 'B B', 'B A', 'A', 'B'],\n",
        "        'gold_label': [0, 1, 0, 1, 0, 1]}\n",
        "    assess_dataset = {\n",
        "        'sentence': ['A C', 'B A'],\n",
        "        'gold_label': [0, 1]}\n",
        "    def featfunc(s):\n",
        "        return Counter(s.split())\n",
        "    model = LogisticRegression()\n",
        "    model, vectorizer = trainfunc(model, featfunc, train_dataset)\n",
        "    result = assessfunc(model, featfunc, vectorizer, assess_dataset)\n",
        "    errcount = 0\n",
        "    if len(vectorizer.vocabulary_) != 2:\n",
        "        print(f\"Error for `{assessfunc.__name__}`: Unexpected feature count\")\n",
        "        errcount += 1\n",
        "    if 'weighted avg' not in result:\n",
        "        print(f\"Error for `{assessfunc.__name__}`: Unexpected return value\")\n",
        "        errcount += 1\n",
        "    if errcount == 0:\n",
        "        print(f\"No errors found for `{assessfunc.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt2debDiN9Kt"
      },
      "outputs": [],
      "source": [
        "test_assess_linear_model(assess_linear_model, train_linear_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tvlMIBhN9Kt"
      },
      "source": [
        "If you trained a model `lr_unigrams` above, you can now easily assess it. An example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buRd2jpYJ8hR"
      },
      "outputs": [],
      "source": [
        "report = assess_linear_model(\n",
        "    lr_unigrams,\n",
        "    unigrams_phi,\n",
        "    vec_unigrams,\n",
        "    dynasent_r1['validation'])\n",
        "\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezZAgHCNJ8hS"
      },
      "source": [
        "## Question 2: Transformer fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjyRRmHrN9Kt"
      },
      "source": [
        "We're now going to move into a more modern mode: fine-tuning pretrained components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QoFFtNrN9Kt"
      },
      "source": [
        "We'll use BERT-mini (originally from [the BERT repo](https://github.com/google-research/bert)) for the homework so that we can rapdily develop prototypes. You can then consider scaling up to larger models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BICLtFMVN9Kt"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4DaVX7bN9Kt"
      },
      "source": [
        "The `transformers` library does a lot of logging. To avoid ending up with a cluttered notebook, I am changing the logging level. You might want to skip this as you scale up to building production systems, since the logging is very good  it gives you a lot of insights into what the models and code are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cR-6tJpN9Kt"
      },
      "outputs": [],
      "source": [
        "transformers.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDHvbWLHN9Ku"
      },
      "source": [
        "Here we set ourselves up to use BERT-mini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbhH76z-N9Ku"
      },
      "outputs": [],
      "source": [
        "weights_name = \"prajjwal1/bert-mini\"\n",
        "\n",
        "bert = AutoModel.from_pretrained(weights_name)\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(weights_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRvmcYjQN9Ku"
      },
      "source": [
        "### Background: Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjwSM8l0N9Ku"
      },
      "source": [
        "Tokenization in Transformer models is handled differently from tokenization in linear models of the sort we used in Question 1. For Transformer models, we need to use the tokenizer that comes with the model so that we reliably have embedding representations for every token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zrjfuPON9Ku"
      },
      "outputs": [],
      "source": [
        "example_text = \"Bert knows Snuffleupagus\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8MNQiqqN9Ku"
      },
      "source": [
        "Here's a basic tokenization step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNauwjYkN9Ku"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer.tokenize(example_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAJA3AkFN9Ku"
      },
      "source": [
        "Notice that the tokenizer split \"Snuffleupagus\" into a bunch of subword tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "330aKpFsN9Ku"
      },
      "source": [
        "The above use of the tokenizer, where we map from strings to lists of strings, is really for us humans. For modeling, the most important step for tokenization is mapping individual strings to sequences of integer ids. These ids key into the lowest embedding layer of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHAtsqC_N9Ku"
      },
      "outputs": [],
      "source": [
        "ex_ids = bert_tokenizer.encode(example_text, add_special_tokens=True)\n",
        "\n",
        "ex_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ontZHYjnN9Ku"
      },
      "source": [
        "We can get map these indices back to \"words\" if we want:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hu6ZCUeN9Ku"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer.convert_ids_to_tokens(ex_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRPfyAKzN9Ku"
      },
      "source": [
        "### Background: Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc0Mu0unN9Ku"
      },
      "source": [
        "Having mapped our string to a list of tokens, we can use the `forward` method of the model to get representations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opWpaeV9N9Ku"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    reps = bert(torch.tensor([ex_ids]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHX_GXIAN9Ku"
      },
      "source": [
        "There are a lot of options for which representations to get. With the above call, we got the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zidzPnGpN9Ku"
      },
      "outputs": [],
      "source": [
        "reps.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWkbFMk9N9Ku"
      },
      "source": [
        "The value of `last_hidden_state` hidden state is the sequence of final output states from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV2hlu13N9Ku"
      },
      "outputs": [],
      "source": [
        "reps.last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFeh3YuxN9Ku"
      },
      "source": [
        "This is: 1 example, 10 token representations, each one a 256 dimension vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulYDkY-CN9Ku"
      },
      "source": [
        "The value of `pooler_output` is a set of currently random parameters sitting on top of the first output hidden state. You can see here that it is a single vector representation per example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_q1-YtIN9Ku"
      },
      "outputs": [],
      "source": [
        "reps.pooler_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppZkw2B-N9Kv"
      },
      "source": [
        "I often feel unsure of precisely what this model component is. Here we can have a quick look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XayUC-T_N9Kv"
      },
      "outputs": [],
      "source": [
        "bert.pooler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x60-obKlN9Kv"
      },
      "source": [
        "So this is a dense linear layer (a single matrix of weights) with a bias term, and a tanh activation function is applied to the output. We could put a classifier head on top of this if we wanted to, but we might have mixed feelings about being stuck with that tanh step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4G296YQN9Kv"
      },
      "source": [
        "### Background: Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwPewR33N9Kv"
      },
      "source": [
        "Where examples from a single batch have different lengths, we need to mask the padded tokens to get the intended results from the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX4-8xlSN9Kv"
      },
      "source": [
        "For a quick example, here we process our full example from above and print out the first five values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OPjrJqXN9Kv"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    reps = bert(torch.tensor([ex_ids]))\n",
        "    print(reps.last_hidden_state[0][0][: 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwDdlaeyN9Kv"
      },
      "source": [
        "And now we do the same thing, but with masking of the final five positions to illustate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm9FJVoCN9Kv"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # Mask the last 5 tokens:\n",
        "    am = torch.tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
        "    maskreps = bert(torch.tensor([ex_ids]), attention_mask=am)\n",
        "    print(maskreps.last_hidden_state[0][0][: 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBL-Sv6kN9Kv"
      },
      "source": [
        "### Task 1: Batch tokenization [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKQssdXsN9Kv"
      },
      "source": [
        "Your task here is to use the `batch_encode_plus` method for `bert_tokenizer` to tokenize a list of strings. You should complete `get_batch_token_ids` according to the specification in the doctring. All these steps can be handled with a single call to `batch_encode_plus`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8yXq7cMN9Kv"
      },
      "outputs": [],
      "source": [
        "def get_batch_token_ids(batch, tokenizer):\n",
        "    \"\"\"Map `batch` to a tensor of ids. The return\n",
        "    value should meet the following specification:\n",
        "\n",
        "    1. The max length should be 512.\n",
        "    2. Examples longer than the max length should be truncated\n",
        "    3. Examples should be padded to the max length for the batch.\n",
        "    4. The special [CLS] should be added to the start and the special\n",
        "       token [SEP] should be added to the end.\n",
        "    5. The attention mask should be returned\n",
        "    6. The return value of each component should be a tensor.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch: list of str\n",
        "    tokenizer: Hugging Face tokenizer\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict with at least \"input_ids\" and \"attention_mask\" as keys,\n",
        "    each with Tensor values\n",
        "\n",
        "    \"\"\"\n",
        "    encoding = tokenizer.batch_encode_plus(\n",
        "        batch,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "    return {\"input_ids\": encoding[\"input_ids\"], \"attention_mask\": encoding[\"attention_mask\"]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muaKvOP5N9Kv"
      },
      "source": [
        "Here's a test you can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-uhZ_glN9Kv"
      },
      "outputs": [],
      "source": [
        "def test_get_batch_token_ids(func):\n",
        "    examples = [\n",
        "        \"Bert knows Snuffleupagus\",\n",
        "        \"ELMo knew Bert.\",\n",
        "        \"Buffalo \" * 520\n",
        "    ]\n",
        "    test_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-mini\")\n",
        "    result = func(examples, test_tokenizer)\n",
        "    errcount = 0\n",
        "    if 'attention_mask' not in result:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Attention mask was not returned\")\n",
        "    ids = result['input_ids']\n",
        "    if not isinstance(ids, torch.Tensor):\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Return values are not tensors\")\n",
        "    if ids.shape[1] != 512:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Expected sequence length 512; got {ids.shape[1]}\")\n",
        "    if ids[0][0] != bert_tokenizer.cls_token_id:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Special tokens were not added\")\n",
        "    if errcount == 0:\n",
        "        print(f\"No errors found for `{func.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNMhQthcN9Kv"
      },
      "outputs": [],
      "source": [
        "test_get_batch_token_ids(get_batch_token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYVtqMOiN9Kv"
      },
      "source": [
        "### Task 2: Contextual representations [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYYMrcJhN9Kv"
      },
      "source": [
        "This next task is not used directly in fine-tuning, but it should help ensure that you understand how BERT representations are created and how they need to be managed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApbCR2zRN9Kv"
      },
      "source": [
        "Your task is to complete `get_reps` so that, given a dataset (list of strings), it returns a single tensor in which each row is the output hidden state above the [CLS] token for that example. `gets_reps` has a batchsize argument that the user can manage depending on how much available memory they have and how large their model is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8hg8iviN9Kv"
      },
      "outputs": [],
      "source": [
        "def get_reps(dataset, model, tokenizer, batchsize=20):\n",
        "    \"\"\"Represent each example in `dataset` with the final hidden state\n",
        "    above the [CLS] token.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : list of str\n",
        "    model : BertModel\n",
        "    tokenizer : BertTokenizerFast\n",
        "    batchsize : int\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor with shape `(n_examples, dim)` where `dim` is the\n",
        "    dimensionality of the representations for `model`\n",
        "\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Iterate over `dataset` in batches:\n",
        "        for i in range(0, len(dataset), batchsize):\n",
        "\n",
        "\n",
        "\n",
        "            # Encode the batch with `get_batch_token_ids`:\n",
        "            batch = dataset[i:i+batchsize]\n",
        "            tokenized = get_batch_token_ids(batch, tokenizer)\n",
        "            tokenized = {key: val.to(model.device) for key, val in tokenized.items()}\n",
        "\n",
        "\n",
        "\n",
        "            # Get the representations from the model, making\n",
        "            # sure to pay attention to masking:\n",
        "            outputs = model(**tokenized)\n",
        "            cls_data = outputs.last_hidden_state[:, 0, :]\n",
        "            data.append(cls_data.cpu())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Return a single tensor:\n",
        "        return torch.cat(data, dim=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkzbyqpJN9Kw"
      },
      "source": [
        "Quick test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf_I7fgZN9Kw"
      },
      "outputs": [],
      "source": [
        "def test_get_reps(func):\n",
        "    examples = [\"The cat slept.\", \"The bird chirped.\"] * 20\n",
        "    weights_name = \"prajjwal1/bert-mini\"\n",
        "    test_model = AutoModel.from_pretrained(weights_name)\n",
        "    test_tokenizer = AutoTokenizer.from_pretrained(weights_name)\n",
        "    result = func(examples, test_model, test_tokenizer, batchsize=2)\n",
        "    errcount = 0\n",
        "    if result.shape != (40, 256):\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Expected shape {(40, 256)}, got {result.shape}\")\n",
        "    if round(result[0][0].item(), 2) != -0.64:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Representations seem to be incorrect\")\n",
        "    if errcount == 0:\n",
        "        print(f\"No errors found for `{func.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4su7jpkTN9Kw"
      },
      "outputs": [],
      "source": [
        "test_get_reps(get_reps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stBiTDoxN9Kw"
      },
      "source": [
        "### Task 3: Fine-tuning module [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6mZSlDTN9Kw"
      },
      "source": [
        "We can now put the above together into a basic `nn.Module` that will fine-tune our BERT model. Most of the module is written for you. The pieces you need to implement:\n",
        "\n",
        "1. in the `init` methid, define `self.classifier_layer` using [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
        "2. Complete the `forward` method.\n",
        "\n",
        "Precise instructions are provided in the docstrings for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI_llzyUJ8hS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BertClassifierModule(nn.Module):\n",
        "    def __init__(self,\n",
        "            n_classes,\n",
        "            hidden_activation,\n",
        "            weights_name=\"prajjwal1/bert-mini\"):\n",
        "        \"\"\"This module loads a Transformer based on  `weights_name`,\n",
        "        puts it in train mode, add a dense layer with activation\n",
        "        function give by `hidden_activation`, and puts a classifier\n",
        "        layer on top of that as the final output. The output of\n",
        "        the dense layer should have the same dimensionality as the\n",
        "        model input.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_classes : int\n",
        "            Number of classes for the output layer\n",
        "        hidden_activation : torch activation function\n",
        "            e.g., nn.Tanh()\n",
        "        weights_name : str\n",
        "            Name of pretrained model to load from Hugging Face\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.weights_name = weights_name\n",
        "        self.bert = AutoModel.from_pretrained(self.weights_name)\n",
        "        self.bert.train()\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n",
        "        # Add the new parameters here using `nn.Sequential`.\n",
        "        # We can define this layer as\n",
        "        #\n",
        "        #  h = f(cW1 + b_h)\n",
        "        #  y = hW2 + b_y\n",
        "        #\n",
        "        # where c is the final hidden state above the [CLS] token,\n",
        "        # W1 has dimensionality (self.hidden_dim, self.hidden_dim),\n",
        "        # W2 has dimensionality (self.hidden_dim, self.n_classes),\n",
        "        # f is the hidden activation, and we rely on the PyTorch loss\n",
        "        # function to add apply a softmax to y.\n",
        "        self.classifier_layer = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "            self.hidden_activation,\n",
        "            nn.Linear(self.hidden_dim, self.n_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, indices, mask):\n",
        "        \"\"\"Process `indices` with `mask` by feeding these arguments\n",
        "        to `self.bert` and then feeding the initial hidden state\n",
        "        in `last_hidden_state` to `self.classifier_layer`\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        indices : tensor.LongTensor of shape (n_batch, k)\n",
        "            Indices into the `self.bert` embedding layer. `n_batch` is\n",
        "            the number of examples and `k` is the sequence length for\n",
        "            this batch\n",
        "        mask : tensor.LongTensor of shape (n_batch, d)\n",
        "            Binary vector indicating which values should be masked.\n",
        "            `n_batch` is the number of examples and `k` is the\n",
        "            sequence length for this batch\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tensor.FloatTensor\n",
        "            Predicted values, shape `(n_batch, self.n_classes)`\n",
        "\n",
        "        \"\"\"\n",
        "        outputs = self.bert(input_ids=indices, attention_mask=mask)\n",
        "        cls_hidden_state = outputs.last_hidden_state[:, 0, :]  # Extract CLS token representation\n",
        "        logits = self.classifier_layer(cls_hidden_state)\n",
        "        return logits\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVTXKOTON9Kw"
      },
      "outputs": [],
      "source": [
        "bert_module = BertClassifierModule(n_classes=3, hidden_activation=nn.Tanh())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtnnZiM6N9Kw"
      },
      "outputs": [],
      "source": [
        "ids = get_batch_token_ids(\n",
        "    dynasent_r1['train']['sentence'][: 2],\n",
        "    bert_tokenizer)\n",
        "\n",
        "bert_module(ids['input_ids'], ids['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phaWCSw-N9Kw"
      },
      "outputs": [],
      "source": [
        "def test_bert_classifier_module(moduleclass):\n",
        "    expected_out = 5\n",
        "    expected_hidden = 256\n",
        "    expected_activation = nn.ReLU()\n",
        "    mod = moduleclass(expected_out, expected_activation)\n",
        "    errcount = 0\n",
        "\n",
        "    # Basic layer structure:\n",
        "    if not hasattr(mod, \"classifier_layer\") or mod.classifier_layer is None:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "              f\"Missing attribute `classifier_layer`\")\n",
        "        return\n",
        "    for i in range(3):\n",
        "        try:\n",
        "            bert_module.classifier_layer[i]\n",
        "        except IndexError:\n",
        "            errcount += 1\n",
        "            print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "                  f\"`classifier_layer` is not an `nn.Sequential` \"\n",
        "                  f\"and/or does not have the right structure\")\n",
        "    # Correct first layer dimensionality:\n",
        "    result_hidden = mod.classifier_layer[0].out_features\n",
        "    if result_hidden != expected_hidden:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "              f\"Expected `classifier_layer` hidden dim {expected_hidden}, \"\n",
        "              f\"got {result_hidden}\")\n",
        "    # Correct activation:\n",
        "    result_activation = mod.classifier_layer[1].__class__.__name__\n",
        "    if result_activation != expected_activation.__class__.__name__:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "              f\"Incorrect hidden activation\")\n",
        "    # Correct output dimensionality:\n",
        "    result_out = mod.classifier_layer[2].out_features\n",
        "    if result_out != expected_out:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "              f\"Expected `classifier_layer` out dim {expected_out}, \"\n",
        "              f\"got {result_out}\")\n",
        "    # forward method:\n",
        "    ids = get_batch_token_ids([\"A B C\", \"A B\"], bert_tokenizer)\n",
        "    result = mod(ids['input_ids'], ids['attention_mask'])\n",
        "    if result.shape != (2, 5):\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "              f\"Expected output shape {(2, 5)}, got {result.shape}\")\n",
        "    if errcount == 0:\n",
        "        print(f\"No errors found for `{moduleclass.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfzuQS5RN9Kw"
      },
      "outputs": [],
      "source": [
        "test_bert_classifier_module(BertClassifierModule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fVzBMDEN9Kw"
      },
      "source": [
        "### Optional use: Classifier interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoUCkX-TN9Kw"
      },
      "source": [
        "The above module doesn't have functionality for processing data and fitting models. Our course code includes some general purpose code for adding these features. Here is an example that should work well with the module you wrote above. For more details on the design of these interfaces, see [tutorial_pytorch_models.ipynb](tutorial_pytorch_models.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfD8yC4AJ8hT"
      },
      "outputs": [],
      "source": [
        "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
        "\n",
        "class BertClassifier(TorchShallowNeuralClassifier):\n",
        "    def __init__(self, weights_name, *args, **kwargs):\n",
        "        self.weights_name = weights_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.weights_name)\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.params += ['weights_name']\n",
        "\n",
        "    def build_graph(self):\n",
        "        return BertClassifierModule(\n",
        "            self.n_classes_, self.hidden_activation, self.weights_name)\n",
        "\n",
        "    def build_dataset(self, X, y=None):\n",
        "        data = get_batch_token_ids(X, self.tokenizer)\n",
        "        if y is None:\n",
        "            dataset = torch.utils.data.TensorDataset(\n",
        "                data['input_ids'], data['attention_mask'])\n",
        "        else:\n",
        "            self.classes_ = sorted(set(y))\n",
        "            self.n_classes_ = len(self.classes_)\n",
        "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
        "            y = [class2index[label] for label in y]\n",
        "            y = torch.tensor(y)\n",
        "            dataset = torch.utils.data.TensorDataset(\n",
        "                data['input_ids'], data['attention_mask'], y)\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXc1te-uN9Kw"
      },
      "source": [
        "And here is a training run that should do pretty well for our problem.\n",
        "\n",
        "__Note__: This step should not be run on CPU machines. On Google Colab with a GPU, it will likely take about an hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re8lzSepJ8hT"
      },
      "outputs": [],
      "source": [
        "bert_finetune = BertClassifier(\n",
        "    weights_name=\"prajjwal1/bert-mini\",\n",
        "    hidden_activation=nn.ReLU(),\n",
        "    eta=0.00005,          # Low learning rate for effective fine-tuning.\n",
        "    batch_size=8,         # Small batches to avoid memory overload.\n",
        "    gradient_accumulation_steps=4,  # Increase the effective batch size to 32.\n",
        "    early_stopping=True,  # Early-stopping\n",
        "    n_iter_no_change=5)   # params."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlvq0G-PJ8hT"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "_ = bert_finetune.fit(\n",
        "    dynasent_r1['train']['sentence'],\n",
        "    dynasent_r1['train']['gold_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwwiIuKqJ8hT"
      },
      "outputs": [],
      "source": [
        "preds = bert_finetune.predict(sst['validation']['sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3Af7RAOJ8hT"
      },
      "outputs": [],
      "source": [
        "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOblLwl3J8hU"
      },
      "outputs": [],
      "source": [
        "preds = bert_finetune.predict(dynasent_r1['validation']['sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndDbPLY1b7lj"
      },
      "outputs": [],
      "source": [
        "print(classification_report(dynasent_r1['validation']['gold_label'], preds, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrOyRrd7N9Kx"
      },
      "source": [
        "## Question 3: Your original system [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOKCytSrN9Kx"
      },
      "source": [
        "Your task is to develop an original ternary sentiment classifier model. There are many options. The only rule:\n",
        "\n",
        "__You cannot make any use of the test sets for DynaSent-R1, DynaSent-R2, or SST-3, at any time during the course of development.__\n",
        "\n",
        "The integrity of the bakeoff depends on this rule being followed.\n",
        "\n",
        "It's fine to use the dev sets for system development  indeed, we encourage this.\n",
        "\n",
        "For system development, here are some relatively manageable ideas that you might try:\n",
        "\n",
        "* Different pretrained models. There are many models available on the [Hugging Face models hub](https://huggingface.co/models) that will be drop-in replacements for BERT-mini as we used it above.\n",
        "\n",
        "* Different fine-tuning regimes. We used the [CLS] token above. This doesn't make especially good use of the output states of the models. Pooling across these representtions (with sum, average, etc.) is likely to be better.\n",
        "\n",
        "* Different training regimes. You have three train sets at your disposal, and there may be other sentiment datasets that could contribute to making your system more robust in new domains.\n",
        "\n",
        "* Entirely different approaches. There is no requirement that you make use of any of the concepts from the homework questions in constructing your original system. Anything goes as long as you follow the one rule given above in bold.\n",
        "\n",
        "We want to emphasize that this needs to be an original system. It doesn't suffice to download code from the Web, retrain, and submit. You can build on others' code, but you have to do something new and meaningful with it. See the course website for additional guidance on how original systems will be evaluated.\n",
        "\n",
        "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UW8Win5N9Kx"
      },
      "outputs": [],
      "source": [
        "# PLEASE MAKE SURE TO INCLUDE THE FOLLOWING BETWEEN THE START AND STOP COMMENTS:\n",
        "#   1) Textual description of your system.\n",
        "#   2) The code for your original system.\n",
        "# PLEASE MAKE SURE NOT TO DELETE OR EDIT THE START AND STOP COMMENTS\n",
        "\n",
        "# This system fine-tunes a transformer-based model for ternary sentiment classification. It uses:\n",
        "# Pretrained Model: distilbert-base-uncased (lighter than BERT, retaining effectiveness).\n",
        "# Pooling Strategy: Instead of just using [CLS], it averages token embeddings.\n",
        "# Augmented Training Data: Utilizes multiple sentiment datasets for robustness.\n",
        "# Fine-tuning Details:\n",
        "# Learning rate scheduling with AdamW\n",
        "# Augmented data with backtranslation\n",
        "# Weighted loss function to address class imbalance\n",
        "# Evaluation Strategy: Assessed on the development sets, ensuring fair comparison.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, AdamW\n",
        "\n",
        "class TernarySentimentClassifier(nn.Module):\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased\", num_classes=3):\n",
        "        super(TernarySentimentClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = torch.mean(outputs.last_hidden_state, dim=1)  # Average pooling\n",
        "        logits = self.classifier(self.dropout(pooled_output))\n",
        "        return logits\n",
        "\n",
        "def train_model(train_dataloader, model, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = (\n",
        "            input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate_model(dev_dataloader, model, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dev_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            correct += (predictions == labels.to(device)).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# STOP COMMENT: Please do not remove this comment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return input_ids, attention_mask, label\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Example dataset (replace with actual sentiment data)\n",
        "train_texts = [\"I love this movie!\", \"This was terrible.\", \"It was okay, not great.\"]\n",
        "train_labels = [2, 0, 1]  # 2 = Positive, 1 = Neutral, 0 = Negative\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "NqLjHudVOk3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKHd0ds7N9Kx"
      },
      "source": [
        "## Question 4: Bakeoff entry [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikSpBRdCN9Kx"
      },
      "source": [
        "The bakeoff dataset is available at\n",
        "\n",
        "https://web.stanford.edu/class/cs224u/data/cs224u-sentiment-test-unlabeled.csv\n",
        "\n",
        "This code should grab it for you and put it in `data/sentiment` if you are working in the cloud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P57FCNj4N9Kx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wget\n",
        "\n",
        "if not os.path.exists(os.path.join(\"data\", \"sentiment\", \"cs224u-sentiment-test-unlabeled.csv\")):\n",
        "    os.makedirs(os.path.join('data', 'sentiment'), exist_ok=True)\n",
        "    wget.download('https://web.stanford.edu/class/cs224u/data/cs224u-sentiment-test-unlabeled.csv', out='data/sentiment/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncrTUhNdN9Kx"
      },
      "source": [
        "If the above fails, you can just download the file and place it in `data/sentiment`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnPzHJhiN9Kx"
      },
      "source": [
        "Once you have the file, you can load it to a `pd.DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vBz3ZIvN9Kx"
      },
      "outputs": [],
      "source": [
        "bakeoff_df = pd.read_csv(\n",
        "    os.path.join(\"data\", \"sentiment\", \"cs224u-sentiment-test-unlabeled.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaWloTkQN9Kx"
      },
      "outputs": [],
      "source": [
        "bakeoff_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6q0u0PvN9Kx"
      },
      "source": [
        "To enter the bakeoff, you simply need to use your original system to:\n",
        "\n",
        "1. Add a column named 'prediction' to `cs224u-sentiment-test-unlabeled.csv` with your model predictions (which are strings in {`positive`, `negative`, `neutral`}). The existing columns should remain.\n",
        "\n",
        "2. Save the file as `cs224u-sentiment-bakeoff-entry.csv`. Here is a good snippet of code for writing this file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fYmgn9ZN9Kx"
      },
      "outputs": [],
      "source": [
        "# This is a placeholder for adding the \"prediction\" column:\n",
        "# bakeoff_df['prediction'] = # Use your model to add predictions.\n",
        "\n",
        "# Write to disk\n",
        "bakeoff_df.to_csv(\"cs224u-sentiment-bakeoff-entry.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcO7avUCN9Kx"
      },
      "source": [
        "In particular, you need to be sure that `example_id` is a column rather than an index when read in by Pandas. Here is a quick test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNBB5hqrN9Kx"
      },
      "outputs": [],
      "source": [
        "def test_bakeoff_entry(filename=\"cs224u-sentiment-bakeoff-entry.csv\"):\n",
        "    gold_df = pd.read_csv(\n",
        "        os.path.join(\"data\", \"sentiment\", \"cs224u-sentiment-test-unlabeled.csv\"))\n",
        "    entry_df = pd.read_csv(filename)\n",
        "\n",
        "    # Check that no required columns are missing:\n",
        "    expected_cols = {'example_id', 'sentence', 'prediction'}\n",
        "    missing_cols = expected_cols - set(entry_df.columns)\n",
        "    errcount = 0\n",
        "    if len(missing_cols) != 0:\n",
        "        errcount += 1\n",
        "        print(f\"Entry is missing required columns {missing_cols}\")\n",
        "        return\n",
        "\n",
        "    # Check that the predictions are in our space:\n",
        "    labels = {'positive', 'negative', 'neutral'}\n",
        "    predtypes = set(entry_df.prediction.unique())\n",
        "    unexpected = predtypes - labels\n",
        "    if len(unexpected) != 0:\n",
        "        errcount += 1\n",
        "        print(f\"Prediction column has unexpected values: {unexpected}\")\n",
        "\n",
        "    # Check that the dataset hasn't been rearranged:\n",
        "    for colname in ('example_id', 'sentence'):\n",
        "        if not entry_df[colname].equals(gold_df[colname]):\n",
        "            errcount += 1\n",
        "            print(f\"Entry is misaligned with test data on column {colname}\")\n",
        "\n",
        "    # Clean bill of health:\n",
        "    if errcount == 0:\n",
        "        print(\"No errors detected with `test_bakeoff_entry`.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzR9qCN3N9Kx"
      },
      "outputs": [],
      "source": [
        "test_bakeoff_entry(\"cs224u-sentiment-bakeoff-entry.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4J7mJvHN9Kx"
      },
      "source": [
        "Submit the following files to Gradescope:\n",
        "\n",
        "* `hw_sentiment.ipynb` (this notebook)\n",
        "* `cs224u-sentiment-bakeoff-entry.csv` (bake-off output)\n",
        "\n",
        "Please make sure you use these filenames. The autograder looks for files with these names.\n",
        "\n",
        "You are not permitted to do any tuning of your system based on what you see in our bakeoff prediction file  you should not study that file in anyway, beyond perhaps checking that it contains what you expected it to contain. The upload function will do some additional checking to ensure that your file is well-formed.\n",
        "\n",
        "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
        "\n",
        "Late entries will be accepted, but they cannot earn the extra 0.5 points."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}